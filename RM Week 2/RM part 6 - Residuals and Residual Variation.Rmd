---
title: "Residuals and Residual Variation"
output:
    html_document:
        keep_md: true
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
require(knitr)
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## Motivating Example

###### `diamond` dataset from `UsingR`

Data is diamond prices (Singapore dollars) and diamond weight in carats (standard measure of diamong mass, 0.2 _g_). To get the data use `library(UsingR); data(diamond)`

---

```{r, echo=FALSE, fig.height=6, fig.width=6}
library(UsingR)
library(ggplot2)
data(diamond)
ggplot(diamond, aes(x=carat, y=price)) +
    xlab("Mass (carats)") +
    ylab("Price (SIN $)") +
    geom_point(size=7, color="black", alpha=0.5) +
    geom_point(size=5, color="blue", alpha=0.2) +
    geom_smooth(method="lm", color="black")
```

---

## Residuals

- Model $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ where $\epsilon_i \sim N\left(0,\sigma^2\right)$
- Observed outcome $i$ is $Y_i$ at predictor value $X_i$
- Predicted outcome $i$ is $\hat Y_i$ at predictor value $X_i$ is
$$
\hat Y_i = \hat\beta_0 + \hat\beta_1 X_i
$$
- Residual, the difference between the observed and predicted outcome
$$
e_i = Y_i - \hat Y_i
$$
    - The vertical distance between the observed data point and the regression line
- Lease squares minimizes $\sum_{i=1}^n e_i^2$
- The $e_i$ can be thought of as estimates of the $\epsilon_i$

---

## Properties of the Residuals

- $E\left[e_i\right] = 0$
- If an intercept is included, $\sum_{i=1}^n e_i = 0$
- If a regressor variable, $X_i$ is included in the model, $\sum_{i=1}^n e_i X_i = 0$
- Residuals are useful for investigating poor model fit
- Positive residuals are above the line, negative residuals are below
- Residuals can be thought of as the outcome ($Y$) with the linear association of the predictor ($X$) removed
- One differentiates residual variation (variation after removing the predictor) from systematic variation (variation explained by the regression model)
- Residual plots highlight poor model fit

---

## Code

```{r}
data(diamond)
y <- diamond$price
x <- diamond$carat
n <- length(y)
fit <- lm(y ~ x)
e <- resid(fit)
yhat <- predict(fit)
max(abs(e - (y - yhat)))
max(abs(e - (y - coef(fit)[1] - coef(fit)[2] * x)))
```

---

## Residuals Are the Signed Length of the Red Lines

```{r, echo=FALSE, fig.height=5, fig.width=5}
plot(diamond$carat, diamond$price,
     xlab="Mass (carats)",
     ylab="Price (SIN $)",
     bg="lightblue",
     col="black", cex=2, pch=21, frame=F)
abline(fit, lwd=2)
for (i in 1:n)
    lines(c(x[i], x[i]), c(y[i],yhat[i]), col="red", lwd=2)
```

---

## Residuals Versus X

```{r, echo=F, fig.height=5, fig.width=5}
plot(x, e,
     xlab="Mass (carats)",
     ylab="Residuals (SIN $)",
     bg="lightblue",
     col="black", cex=2, pch=21, frame=F)
abline(h=0, lwd=2)
for (i in 1:n)
    lines(c(x[i],x[i]), c(e[i], 0), col="red", lwd=2)
```

---

## Non-linear Data

```{r, echo=FALSE, fig.height=5, fig.width=5}
x <- runif(100, -3, 3)
y <- x + sin(x) + rnorm(100, sd=.2)
library(ggplot2)
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) +
    geom_smooth(method="lm", color="black") +
    geom_point(size=7, color="black", alpha=0.4) +
    geom_point(size=5, color="red", alpha=0.4)
```

---

## Residual Plot

```{r, echo=FALSE, fig.height=5, fig.width=5}
ggplot(data.frame(x=x, y=resid(lm(y ~ x))), aes(x=x, y=y)) +
    geom_hline(yintercept=0, size=2) +
    geom_point(size=7, color="black", alpha=0.4) +
    geom_point(size=5, color="red", alpha=0.4) +
    xlab("X") + ylab("Residual")
```

---

## Heteroskedasticity

```{r, echo=FALSE, fig.height=4.5, fig.width=4.5}
x <- runif(100, 0, 6)
y <- x + rnorm(100, mean=0, sd=.001 * x)
ggplot(data.frame(x=x, y=y), aes(x=x, y=y)) +
    geom_smooth(method="lm", color="black") +
    geom_point(size=7, color="black", alpha=0.4) +
    geom_point(size=5, color="red", alpha=0.4)
```

---

## Getting Rid of the Blank Space can be helpful

```{r, echo=FALSE, fig.height=4.5, fig.width=4.5}
ggplot(data.frame(x=x, y=resid(lm(y ~ x))), aes(x=x, y=y)) +
    geom_hline(yintercept=0, size=2) +
    geom_point(size=7, color="black", alpha=0.4) +
    geom_point(size=5, color="red", alpha=0.4) +
    xlab("X") +
    ylab("Residual")
```

---

## Diamond Data Residual Plot

```{r, echo=F, fig.height=4.5, fig.width=4.5}
diamond$e <- resid(lm(price ~ carat, data=diamond))
ggplot(diamond, aes(x=carat, y=e)) +
    geom_hline(yintercept=0, size=2) +
    geom_point(size=7, color="black", alpha=0.5) +
    geom_point(size=5, color="blue", alpha=0.2) +
    xlab("Mass (carats)") +
    ylab("Residual price (SIN $)")
```

---

## Diamond Data Residual Plot

```{r, echo=F, fig.height=4.5, fig.width=4.5}
e <- c(resid(lm(price ~ 1, data=diamond)),
       resid(lm(price ~ carat, data=diamond)))
fit <- factor(c(rep("Itc", nrow(diamond)),
                rep("Itc, slope", nrow(diamond))))
ggplot(data.frame(e=e, fit=fit), aes(y=e, x=fit, fill=fit)) +
    geom_dotplot(binaxis="y", size=2, stackdir="center", binwidth=20) +
    xlab("Fitting Approach") +
    ylab("Residual Price")
```

---

## Estimating Residual Variation

- Model $Y_i = \beta_0 + \beta_1 X_i + \epsilon_i$ where $\epsilon_i \sim N\left(0,\sigma^2\right)$
- The ML estimate of $\sigma^2$ is $\frac{1}{n} \sum_{i=1}^n e_i^2$, the average squared residual
- Most people use
$$
\hat\sigma^2 = \frac{1}{n - 2}\sum_{i=1}^n e_i^2
$$
- The $n - 2$ instead of $n$ so that $E\left[\hat\sigma^2\right] = \sigma^2$

---

## Diamond Example

```{r}
y <- diamond$price
x <- diamond$carat
n <- length(y)
fit <- lm(y ~ x)
summary(fit)$sigma
sqrt(sum(resid(fit)^2) / (n - 2))
```

---

## Summarizing Variation

- The total variability in our response is the variability around an intercept (think mean-only regression) $\sum_{i=1}^n \left(Y_i - \bar Y\right)^2$
- The regression variability is the variability that is explained by adding the predictor $\sum_{i=1}^n \left(\hat Y_i - \bar Y\right)^2$
- The error variability is what's leftover around the regression line $\sum_{i=1}^n \left(Y_i - \hat Y_i\right)^2$
- Neat fact
$$
\sum_{i=1}^n\left(Y_i - \bar Y\right)^2 = \sum_{i=1}^n\left(Y_i - \hat Y_i\right)^2 + \sum_{i=1}^n\left(\hat Y_i - \bar Y\right)^2
$$

---

## R Squared

- R Squared is the percentage of the total variability that is explained by the linear relationship with the predictor
$$
R^2 = \frac{\sum_{i=1}^n \left(\hat Y_i - \bar Y\right)^2}{\sum_{i=1}^n \left(Y_i - \bar Y\right)^2}
$$

---

## Some Facts about $R^2$

- $R^2$ is the percentage of variation explained by the regression model
- $0 \le R^2 \le 1$
- $R^2$ is the sample correlation squared
- $R^2$ can be a misleading summary of model fit
    - Deleting data can inflate $R^2$
    - (For later.) Adding terms to a regression model always increases $R^2$
- Do `example(anscombe)` to see the following data
    - Basically same mean and variance of $X$ and $Y$
    - Identical correlations (hence same $R^2$)
    - Same linear regression relationship

---

## `data(anscombe); example(anscombe)`

```{r, echo=T, fig.height=5, fig.width=5}
require(stats)
require(graphics)
data(anscombe)
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for (i in 1:4) {
    ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
    mods[[i]] <- lmi <- lm(ff, data=anscombe)
    print(anova(lmi))
}
```

---

## Now, do what you should have done in the first place: PLOTS

```{r, echo=T, fig.height=5, fig.width=5}
op <- par(mfrow=c(2,2), mar=0.1+c(4,4,1,1), oma=c(0,0,2,0))
for(i in 1:4) {
    ff[2:3] <- lapply(paste0(c("y","x"),i), as.name)
    plot(ff, data=anscombe, col="red", pch=21, bg="orange", cex=1.2,
         xlim=c(3,19), ylim=c(3,13))
    abline(mods[[i]], col="blue")
}
mtext("Anscombe's 4 Regression Datasets", outer=T, cex=1.5)
par(op)
```

---

## How to Derive $R^2$ (Not required!)

###### For those that are interested
$$
\begin{eqnarray*}
\sum_{i=1}^n \left(Y_i - \bar Y\right)^2 \
& = & \sum_{i=1}^n \left(Y_i - \hat Y_i + \hat Y_i - \bar Y\right)^2 \\
& = & \sum_{i=1}^n \left(Y_i - \hat Y_i\right)^2 + 2\sum_{i=1}^n \left(Y_i - \hat Y_i\right)\left(\hat Y_i - \bar Y\right) + \sum_{i=1}^n \left(\hat Y_i - \bar Y\right)^2
\end{eqnarray*}
$$

---

## The Relationship Between $R^2$ and _r_

###### Again, not required

Recall that $\left(\hat Y_i - \bar Y\right) = \hat\beta_1\left(X_i - \bar X\right)$ so that

$$
R^2 = \frac{\sum_{i=1}^n \left(\hat Y_i - \bar Y\right)^2}{\sum_{i=1}^n \left(Y_i - \bar Y\right)^2} = \hat\beta_1^2 \frac{\sum_{i=1}^n \left(X_i - \bar X\right)^2}{\sum_{i=1}^n \left(Y_i - \bar Y\right)^2} = Cor\left(Y,X\right)^2
$$

Since, recall,

$$
\hat\beta_1 = Cor\left(Y,X\right) \frac{Sd\left(Y\right)}{Sd\left(X\right)}
$$

So, $R^2$ is literally $r$ squared