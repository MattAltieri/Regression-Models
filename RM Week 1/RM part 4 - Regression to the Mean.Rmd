---
title: 'Historical Side Note: Regression to Mediocrity'
output: 
    html_document:
        keep_md: true
---

```{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE}
require(knitr)
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
```

## A Historically Famous Idea, Regression to the Mean

- Why is it that the children of tall parents tend to be tall, but not as tall as their parents?
- Why do children of short parents tend to be short, but not as short as their parents?
- Why do parents of very short children tend to be short, but not as short as their children? And the same with parents of very tall children?
- Why do the best performing athletes this year tend to do a little worse the following year?

---

## Regression to the Mean

- These phenomena are all examples of the so-called regression to the mean
- Invented by Francis Galton in the paper "Regression Towards Mediocrity in Hereditary Stature", The Journal of the Anthropological Institute of Great Britain and Ireland, Vol. 15 (1886)
- Think of it this way: imagine if you simulated pairs of random normals
    - The largest first ones would be the largest by chance, and the probability that there are smaller for the second simulation is high
    - In other words $P\left(Y < x|X = x\right)$ gets bigger as $x$ heads into the very large values
    - Similarly $P\left(Y > x|X = x\right)$ gets bigger as $x$ heads to very small values
- Think of the regression line as the intrinsic part
    - Unless $Cor\left(Y,X\right) = 1$ the intrinsic part isn't perfect
    
---

## Regression to the Mean

- Suppose that we normalize $X$ (child's height) and $Y$ (parent's height) so that they both have mean 0 and variance 1
- Then, recall, our regression line passes through $\left(0,0\right)$ (the mean of $X$ and $Y$)
- If the slope of the regression line is $Cor\left(Y,X\right)$, regardless of which variable is the outcome (recall, both standard deviation 1)
- Notice if $X$ is the outcome and you create a plot where $X$ is the horizontal axis, the slope of the least squares line that you plot is $1/Cor\left(Y,X\right)$

---

## Plot of the Results

```{r, fig.height=6, fig.width=6, echo=FALSE, fig.align='center'}
library(UsingR)
data(father.son)
y <- (father.son$sheight - mean(father.son$sheight)) / sd(father.son$sheight)
x <- (father.son$fheight - mean(father.son$fheight)) / sd(father.son$fheight)
rho <- cor(x, y)
library(ggplot2)
ggplot(data.frame(x, y), aes(x=x, y=y)) +
    geom_point(size=5, alpha=.2, color="black") +
    geom_point(size=4, alpha=.2, color="red") +
    geom_vline(xintercept=0) +
    geom_hline(yintercept=0) +
    geom_abline(position="identity") +
    geom_abline(intercept=0, slope=rho, size=2) +
    geom_abline(intercept=0, slope=1 / rho, size=2) +
    xlab("Father's height, normalized") +
    ylab("Son's height, normalized")
```

---

## Discussion

- If you had to predict a son's normalized height, it would be $Cor\left(Y,X\right) \times X_i$
- If you had to predict a father's normalized height, it would be $Cor\left(Y,X\right) \times Y_i$
- Multiplication by this correlation shrinks toward 0 (regression toward the mean)
- If the correlation is 1 there is no regression to the mean (if father's height perfectly determines child's height and vice versa)
- Note, regression to the mean has been thought about quite a bit and generalized