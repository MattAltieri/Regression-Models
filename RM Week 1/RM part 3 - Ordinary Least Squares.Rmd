---
title: "Least Squares Estimation of Regression Lines"
output:
    html_document:
        keep_md: true
---

````{r setup, cache = FALSE, echo = FALSE, message = FALSE, warning = FALSE, tidy = FALSE, results='hide', error=FALSE}
require(knitr)
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')

options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
  if(is.numeric(x)) {
    round(x, getOption('digits'))
  } else {
    paste(as.character(x), collapse = ', ')
  }
})
knit_hooks$set(plot = knitr:::hook_plot_html)
runif(1)
```

## General Least Squares for Linear Equations

Consider again the parent and child height data from Galton

```{r, fig.height=5, fig.width=8, echo=FALSE}
library(UsingR)
data(galton)
library(dplyr)
library(ggplot2)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
ggplot(filter(freqData, freq > 0), aes(x=parent, y=child)) +
    geom_point(color="grey50", aes(size=freq+20, show_guide=F)) +
    geom_point(aes(color=freq, size=freq)) +
    scale_color_gradient(low="lightblue", high="white")
```

---

## Fitting the Best Line

- Let $Y_i$ be the $i^{th}$ child's height and $X_i$be the $i^{th}$ (average over the pair of) parents' heights
- Consider finding the best line
    - Child's height = $\beta_0$ + parent's height $\beta_1$
- Use least squares

$$
\sum_{i=1}^n \left\{Y_i - \left(\beta_0 + \beta_1 X_i\right)\right\}^2
$$

---

## Results

- The least squares model fit to the line $Y = \beta_0 + \beta_ X$ through the data pairs $\left(X_i,Y_i\right)$ with $Y_i$ as the outcome obtains the line $Y = \hat \beta_0 + \hat \beta_1 X$ where

$$
\begin{eqnarray*}
\hat \beta_1 \
& = & Cor\left(Y,X\right) \frac{Sd\left(Y\right)}{Sd\left(X\right)} \hat \beta_0 \\
& = & \bar Y - \hat \beta_1 \bar X
\end{eqnarray*}
$$

- $\hat \beta_1$ has the units of $Y/X$, $\hat \beta_0$ has the units of $Y$
- The line passes through the point $\left(\bar X,\bar Y\right)$
- The slope of the regression line with $X$ as the outcome and $Y$ as the predictor is $Cor\left(Y,X\right)Sd\left(X\right)/Sd\left(Y\right)$
- The slope is the same one you would get if you centered the data, $\left(X_i - \bar X,Y_i - \bar Y\right)$ and did regression through the origin
- If you normalized the data, $\left\{\frac{X_i - \bar X}{Sd\left(X\right)},\frac{Y_i - \bar Y}{Sd\left(Y\right)}\right\}$, the slope is $Cor\left(Y,X\right)$

---

## Revisiting Galton's Data

###### Double check our calculations using R

```{r}
y <- galton$child
x <- galton$parent
beta1 <- cor(y, x) * sd(y) / sd(x)
beta0 <- mean(y) - beta1 * mean(x)
rbind(c(beta0, beta1), coef(lm(y ~ x)))
```

---

###### Reversing the outcome/predictor relationship

```{r}
beta1 <- cor(y, x) * sd(x) / sd(y)
beta0 <- mean(x) - beta1 * mean(y)
rbind(c(beta0, beta1), coef(lm(x ~ y)))
```

---

###### Regression through the origin yields an equivalent slope if you center the data first

```{r}
yc <- y - mean(y)
xc <- x - mean(x)
beta1 <- sum(yc * xc) / sum(xc^2)
c(beta1, coef(lm(y ~ x))[2])
```

---

###### Normalizing variables results in the slope being the correlation

```{r}
yn <- (y - mean(y))/sd(y)
xn <- (x - mean(x))/sd(x)
c(cor(y, x), cor(yn, xn), coef(lm(yn ~ xn))[2])
```

---

```{r, fig.height=6, fig.width=6, echo=FALSE}
ggplot(filter(freqData, freq > 0), aes(x=parent, y=child)) +
    geom_point(color="grey50", aes(size=freq + 20, show_guide=F)) +
    geom_point(aes(color=freq, size=freq)) +
    scale_color_gradient(low="lightblue", high="white") +
    geom_smooth(method="lm", formula=y ~ x)
```