---
title: "Residuals, Diagnostics, & Variation"
output:
    html_document:
        keep_md: true
---

```{r setup, cache=FALSE, echo=FALSE, message=F, warning=F, tidy=FALSE}
require(knitr)
options(width=100)
opts_chunk$set(message=F, error=F, warning=F, comment=NA, fig.align='center', dpi=100, tidy=F, cache.path='.cache/', fig.path='fig/')

options(xtable.type='html')
knit_hooks$set(inline=function(x) {
    if(is.numeric(x)) {
        round(x, getOptions('digits'))
    } else {
        paste(as.character(x), collapse=', ')
    }
})
knit_hooks$set(plot=knitr:::hook_plot_html)
```

## The Linear Model

- Specified as $Y_i = \sum_{k=1}^p X_{ik} \beta_j + \epsilon_i$
- We'll also assume here that $\epsilon_i \stackrel{iid}{\sim} N\left(0, \sigma^2\right)$
- Define the residuals as $e_i = Y_i - \hat Y_i = Y_i - \sum_{k=1}^p X_{ik} \hat\beta_j$
- Our estimate of residual variation is $\hat\sigma^2 = \frac{\sum_{i=1}^n e_i^2}{n - p}$, the $n - p$ so that $E\left[\hat\sigma^2\right] = \sigma^2$

---

```{r, fig.height=5, fig.width=5, results='hide'}
data(swiss)
par(mfrow=c(2,2))
fit <- lm(Fertility ~ ., swiss)
plot(fit)
```

---

## Influential, High Leverage, and Outlying Points

```{r, fig.height=5, fig.width=5, echo=FALSE, results='hide'}
n <- 100
x <- rnorm(n)
y <- x + rnorm(n, sd=.3)
plot(c(-3,6), c(-3,6), type="n", frame=F, xlab="X", ylab="Y")
abline(lm(y ~ x), lwd=2)
points(x, y, cex=2, bg="lightblue", col="black", pch=21)
points(0, 0, cex=2, bg="darkorange", col="black", pch=21)
points(0, 5, cex=2, bg="darkorange", col="black", pch=21)
points(5, 5, cex=2, bg="darkorange", col="black", pch=21)
points(5, 0, cex=2, bg="darkorange", col="black", pch=21)
```

---

## Summary of the Plot

Calling a point an outlier is vauge. Instead...

**Leverage:** How far out from the mean (to the right or left) a data point has (if the mean is the fulcrum, a data point far away from it has more leverage)

**Influence:** How far off the regression line a data point w/ high leverage is. If it's on the regression line, it has high leverage but low influence. If it's far off the line (and far outside the x range of the data) it has both high leverage and high influence.

Basically, these points cause the regression line to rotate away from the best fit.

- Outliers can be the result of spurious or real processes
- Outliers can have varying degrees of influence
- Outliers can conform to the regression relationship (i.e. being marginally outlying in $X$ or $Y$, but not outlying given the regression relationship)
    - Upper left hand point has low leverage, low influence, outlies in a way not conforming to the regression relationship
    - Lower left hand point has low leverage, low influence, and is not to be an outlier in any sense
    - Upper right hand point has high leverage, but chooses not to exert it and thus would have low actual influence by conforming to the regression relationship
    - Lower right hand point has high leverege and would exert it if it were included in the fit
    
---

## Influence Measures

- Do `?influence.measures` to see the full suite of influence measures in stats. The measures include:
    - Residuals
        - `rstandard` - standardized residuals, residuals divided by their standard deviation
        - `rstudent` - standardized residuals, residuals divided by their standard deviations, where the $i^{th}$ data point was deleted in the calculation of the standard deviation for the residual to follow a $t$ distribution
        - `resid` - returns the ordinary residuals
        - `resid(fit) / (1 - hatvalues(fit))` where `fit` is the linear model fit. Returns the PRESS residuals, i.e. the leave-out-one cross-validation residuals - the difference in the response and the predicted response at data point $i$ where it was not included in the model fitting
    - Leverage
        - `hatvalues` - measures the leverage
    - Influence
        - `dffits` - change in the predicted response when the $i^{th}$ point is deleted in fitting the model
        - `dfbetas` - change in the individual coefficients when the $i^{th}$ point is deleted in fitting the model
        - `cooks.distance` - overall change in the coefficients when the $i^{th}$ point is deleted
    
---

## How Do I Use All of These Things?

- Be wary of simplistic rules for diagnostic plots and measures. The use of these tools is context specific. It's better to understand what they are trying to accomplish and use them judiciously.
- Not all of the measures have meaningful absolute scales. You can look at them relative to the values across the data.
- They probe your data in different ways to diagnose different problems.
- Patterns in your residual plots generally indicate some poor aspect of model fit. These can include:
    - Heteroskedasticity (non-constant variance)
    - Missing model terms
    - Temporal patterns (plot residuals versus collection order)
- Residual QQ plots investigate normality of the errors
- Leverage measures (hat values) can be useful for diagnosing data entry errors.
- Influence measures get to the bottom line, "how does deleting or including this point impact a particular aspect of the model?"

---

## Case 1

```{r, fig.height=5, fig.width=5, echo=FALSE}
x <- c(10, rnorm(n))
y <- c(10, c(rnorm(n)))
plot(x, y, frame=F, cex=2, pch=21, bg="lightblue", col="black")
abline(lm(y ~ x))
```

---

## The Code

```
x <- c(10, rnorm(n))
y <- c(10, c(norm(n)))
plot(x, y, frame=F, cex=2, pch=21, bg="lightblue", col="black")
abline(lm(y ~ x))
```

---

## Showing a Couple of the Diagnostic Values

```{r}
fit <- lm(y ~ x)
round(dfbetas(fit)[1:10,2], 3)
round(hatvalues(fit)[1:10],3)
```

---

## Case 2

```{r, fig.height=5, fig.width=5, echo=FALSE}
x <- rnorm(n)
y <- x + rnorm(n, sd=.3)
x <- c(5, x)
y <- c(5, y)
plot(x, y, frame=F, cex=2, pch=21, bg="lightblue", col="black")
fit2 <- lm(y ~ x)
abline(fit2)
```

---

## Looking at Some of the Diagnostics

```{r}
round(dfbetas(fit2)[1:10,2], 3)
round(hatvalues(fit2)[1:10], 3)
```

---

## Example Described by Stefanski TAS 2007 Vol 61.

```{r, fig.height=4, fig.width=4}
dat <- read.table('http://www4.stat.ncsu.edu/~stefanski/NSF_Supported/Hidden_Images/orly_owl_files/orly_owl_Lin_4p_5_flat.txt', header = FALSE)
pairs(dat)
```

---

## Got Our $p$-values, Should We Bother To Do a Residual Plot?

```{r}
summary(lm(V1 ~ . - 1, dat))$coef
```

---

## Residual Plot

```{r}
fit <- lm(V1 ~ . - 1, dat)
plot(predict(fit), resid(fit), pch='.')
```

---

## Back to the Swiss Data

```{r, fig.height=5, fig.width=5, echo=FALSE}
par(mfrow=c(2,2))
fit <- lm(Fertility ~ ., swiss)
plot(fit)
```